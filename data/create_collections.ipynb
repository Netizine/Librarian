{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27cec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zk66/.conda/envs/regal_new/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom.mplstyle not found. Using default style.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "from chromadb.utils import embedding_functions\n",
    "from radon.raw import analyze \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, MiniBatchKMeans\n",
    "import hdbscan\n",
    "\n",
    "\n",
    "\n",
    "if os.path.exists(\"./custom.mplstyle\"):\n",
    "    plt.style.use(\"./custom.mplstyle\")\n",
    "else:\n",
    "    print(\"custom.mplstyle not found. Using default style.\")\n",
    "    plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13ead35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"deepmind/code_contests\")\n",
    "PYTHON = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dfffab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 3474 samples because their cf_tags was [''].\n",
      "Filtered 5519 Python 3 samples from the training set.\n"
     ]
    }
   ],
   "source": [
    "def filter_samples(dataset_split, language, max_samples=100000):\n",
    "    filtered_samples = []\n",
    "    skipped_for_empty_tag = 0\n",
    "\n",
    "    for sample in dataset_split:\n",
    "        # New condition: Check if 'cf_tags' exists and is ['']\n",
    "        if \"cf_tags\" in sample and sample[\"cf_tags\"] == ['']:\n",
    "            skipped_for_empty_tag += 1\n",
    "            continue  \n",
    "\n",
    "        # Original logic for language filtering starts here\n",
    "        if \"solutions\" not in sample or not isinstance(sample[\"solutions\"], dict):\n",
    "            print(\"Warning: Sample 'solutions' is not a dict. Skipping.\")\n",
    "            continue\n",
    "        if \"solution\" not in sample[\"solutions\"] or \"language\" not in sample[\"solutions\"]:\n",
    "            print(f\"Warning: Sample 'solutions' dict missing 'solution' or 'language' key. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        solutions = sample[\"solutions\"][\"solution\"]\n",
    "        languages = sample[\"solutions\"][\"language\"]\n",
    "\n",
    "        # Ensure solutions and languages are lists and have the same length\n",
    "        if not (isinstance(solutions, list) and \n",
    "                isinstance(languages, list) and \n",
    "                len(solutions) == len(languages)):\n",
    "            print(f\"Warning: Malformed solutions/languages (not parallel lists). Skipping sample.\")\n",
    "            continue\n",
    "\n",
    "        for i, lang in enumerate(languages):\n",
    "            if lang == language:\n",
    "                new_sample = copy.deepcopy(sample)\n",
    "                # Overwrite the solutions field to only have the selected solution\n",
    "                new_sample[\"solutions\"] = {\n",
    "                    \"solution\": solutions[i],\n",
    "                    \"language\": language\n",
    "                }\n",
    "                filtered_samples.append(new_sample)\n",
    "                break \n",
    "        \n",
    "        if len(filtered_samples) >= max_samples: # Use >= just in case, though == works\n",
    "            break\n",
    "\n",
    "    if skipped_for_empty_tag > 0:\n",
    "        print(f\"Skipped {skipped_for_empty_tag} samples because their cf_tags was [''].\")\n",
    "    return filtered_samples\n",
    "\n",
    "python3_train = filter_samples(dataset[\"train\"], PYTHON, 100000)\n",
    "print(f\"Filtered {len(python3_train)} Python 3 samples from the training set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73fe5c9",
   "metadata": {},
   "source": [
    "## Filter by SLOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d3c6f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def filter_by_sloc(samples_list, sloc_threshold):\n",
    "    \"\"\"\n",
    "    Filters a list of code samples, keeping only those where the solution's\n",
    "    Source Lines of Code (SLOC) is greater than the specified threshold.\n",
    "\n",
    "    Assumes each sample in the list is a dictionary with a structure like:\n",
    "    {\"solutions\": {\"solution\": \"python code string\", \"language\": \"Python 3\"}, ...}\n",
    "\n",
    "    Args:\n",
    "        samples_list (list): The list of samples to filter (e.g., python3_train).\n",
    "        sloc_threshold (int): The SLOC value. Samples with SLOC strictly greater\n",
    "                              than this value will be kept.\n",
    "\n",
    "    Returns:\n",
    "        list: A new list containing only the filtered samples.\n",
    "    \"\"\"\n",
    "    filtered_list = []\n",
    "    processed_count = 0\n",
    "    kept_count = 0\n",
    "    skipped_key_error = 0\n",
    "    skipped_analysis_error = 0\n",
    "    original_count = len(samples_list)\n",
    "\n",
    "    print(f\"\\nFiltering {original_count} samples by SLOC > {sloc_threshold}...\")\n",
    "\n",
    "    for i, sample in enumerate(samples_list):\n",
    "        python_code = None # Define in outer scope for error message\n",
    "        try:\n",
    "            # Assumes the structure created by your filter_samples function\n",
    "            python_code = sample[\"solutions\"][\"solution\"]\n",
    "\n",
    "            # Basic type check (optional but good practice)\n",
    "            if not isinstance(python_code, str):\n",
    "                 print(f\"  Warning: Sample {i} 'solution' is not a string (Type: {type(python_code)}). Skipping.\")\n",
    "                 skipped_key_error += 1\n",
    "                 continue\n",
    "\n",
    "            sloc = 0\n",
    "            try:\n",
    "                if not python_code or python_code.isspace():\n",
    "                    sloc = 0\n",
    "                else:\n",
    "                    analysis = analyze(python_code)\n",
    "                    sloc = analysis.sloc\n",
    "                processed_count += 1\n",
    "\n",
    "            except Exception as analysis_e:\n",
    "                print(f\"  Warning: Radon analysis failed for sample {i}: {analysis_e}. Code snippet: '{str(python_code)[:50]}...'. Skipping sample.\")\n",
    "                skipped_analysis_error += 1\n",
    "                continue \n",
    "\n",
    "            if sloc > sloc_threshold:\n",
    "                filtered_list.append(sample)\n",
    "                kept_count += 1\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"  Warning: Skipping sample {i} due to missing key: {e}. Sample structure: {list(sample.keys())}\")\n",
    "            skipped_key_error += 1\n",
    "        except Exception as e: \n",
    "            print(f\"  Warning: Unexpected error processing sample {i}: {e}. Skipping.\")\n",
    "            skipped_key_error += 1\n",
    "\n",
    "\n",
    "    print(f\"Filtering complete.\")\n",
    "    print(f\"  Successfully analyzed: {processed_count}\")\n",
    "    print(f\"  Kept (SLOC > {sloc_threshold}): {kept_count}\")\n",
    "    print(f\"  Skipped (Key/Type Error): {skipped_key_error}\")\n",
    "    print(f\"  Skipped (Radon Analysis Error): {skipped_analysis_error}\")\n",
    "    print(f\"  Total original samples: {original_count}\")\n",
    "    \n",
    "    if original_count != kept_count + (processed_count - kept_count) + skipped_key_error + skipped_analysis_error:\n",
    "         print(f\"  Line count check: Original ({original_count}) vs Kept ({kept_count}) + Filtered ({processed_count - kept_count}) + Skipped ({skipped_key_error + skipped_analysis_error}) = {kept_count + (processed_count - kept_count) + skipped_key_error + skipped_analysis_error}\")\n",
    "\n",
    "\n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bb57e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering 5519 samples by SLOC > 10...\n",
      "Filtering complete.\n",
      "  Successfully analyzed: 5519\n",
      "  Kept (SLOC > 10): 4596\n",
      "  Skipped (Key/Type Error): 0\n",
      "  Skipped (Radon Analysis Error): 0\n",
      "  Total original samples: 5519\n",
      "\n",
      "Original number of Python 3 samples: 5519\n",
      "Filtered number of samples (SLOC > 10): 4596\n"
     ]
    }
   ],
   "source": [
    "SLOC_MIN_THRESHOLD = 10\n",
    "\n",
    "python3_train_sloc_filtered = filter_by_sloc(python3_train, SLOC_MIN_THRESHOLD)\n",
    "\n",
    "print(f\"\\nOriginal number of Python 3 samples: {len(python3_train)}\")\n",
    "print(f\"Filtered number of samples (SLOC > {SLOC_MIN_THRESHOLD}): {len(python3_train_sloc_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd7bbb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['name', 'description', 'public_tests', 'private_tests', 'generated_tests', 'source', 'difficulty', 'solutions', 'incorrect_solutions', 'cf_contest_id', 'cf_index', 'cf_points', 'cf_rating', 'cf_tags', 'is_description_translated', 'untranslated_description', 'time_limit', 'memory_limit_bytes', 'input_file', 'output_file'])\n",
      "['*special', 'constructive algorithms']\n",
      "\n",
      "Filtered number of samples (SLOC > 10): 4596\n"
     ]
    }
   ],
   "source": [
    "print(python3_train[0].keys())\n",
    "print(python3_train[500][\"cf_tags\"])\n",
    "\n",
    "python3_train = python3_train_sloc_filtered\n",
    "\n",
    "print(f\"\\nFiltered number of samples (SLOC > {SLOC_MIN_THRESHOLD}): {len(python3_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d48e96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8139\n",
      "8139\n"
     ]
    }
   ],
   "source": [
    "def load_json_entries(filename):\n",
    "    \"\"\"Load JSON list from a file.\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "descriptions_path = \"LLM_descriptions.json\"\n",
    "descriptions = load_json_entries(descriptions_path)\n",
    "\n",
    "def transform_to_dict(data):\n",
    "    transformed_data = {}\n",
    "    for item in data:\n",
    "        key = item[0]\n",
    "        value = item[1]\n",
    "        transformed_data[key] = value\n",
    "    return transformed_data\n",
    "\n",
    "descriptions_dict = transform_to_dict(descriptions)\n",
    "print(len(descriptions_dict))\n",
    "\n",
    "def remove_hyphen_prefix(d):\n",
    "    new_d = {}\n",
    "    for key, value in d.items():\n",
    "        # Split at the first hyphen and keep the part after it\n",
    "        if '-' in key:\n",
    "            new_key = key.split('-', 1)[1]\n",
    "        else:\n",
    "            new_key = key  # in case there's no '-'\n",
    "        new_d[new_key] = value\n",
    "    return new_d\n",
    "\n",
    "d = remove_hyphen_prefix(descriptions_dict)\n",
    "print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c3411f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in old_descriptions: 4596\n",
      "Number of entries in new_descriptions: 4596\n",
      "-----------------------------\n",
      "1. One-sentence summary:  \n",
      "   The solution incrementally maintains the size of the k-core of an undirected graph as edges are removed one by one.\n",
      "\n",
      "2. Core algorithmic approach:  \n",
      "   Greedy k-core peeling using a queue (BFS-style removal of vertices whose degree falls below k), applied in reverse over the sequence of edge deletions.\n",
      "\n",
      "3. Reusable components:  \n",
      "   a. k-core peeling routine: a function that, given a starting queue of “low-degree” vertices, repeatedly pops a vertex, decrements its neighbors’ degrees, and enqueues any that drop below k.  \n",
      "   b. Degree‐tracking array: an array that holds current degrees (or “remaining neighbor counts”) for all vertices, supporting O(1) updates.  \n",
      "   c. Reverse-operation pattern: processing a sequence of removals (or additions) in reverse, recording intermediate results as you “undo” operations efficiently.\n",
      "-----------------------------\n",
      "There are n persons who initially don't know each other. On each morning, two of them, who were not friends before, become friends.\n",
      "\n",
      "We want to plan a trip for every evening of m days. On each trip, you have to select a group of people that will go on the trip. For every person, one of the following should hold: \n",
      "\n",
      "  * Either this person does not go on the trip, \n",
      "  * Or at least k of his friends also go on the trip. \n",
      "\n",
      "\n",
      "\n",
      "Note that the friendship is not transitive. That is, if a and b are friends and b and c are friends, it does not necessarily imply that a and c are friends.\n",
      "\n",
      "For each day, find the maximum number of people that can go on the trip on that day.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains three integers n, m, and k (2 ≤ n ≤ 2 ⋅ 10^5, 1 ≤ m ≤ 2 ⋅ 10^5, 1 ≤ k < n) — the number of people, the number of days and the number of friends each person on the trip should have in the group.\n",
      "\n",
      "The i-th (1 ≤ i ≤ m) of the next m lines contains two integers x and y (1≤ x, y≤ n, x≠ y), meaning that persons x and y become friends on the morning of day i. It is guaranteed that x and y were not friends before.\n",
      "\n",
      "Output\n",
      "\n",
      "Print exactly m lines, where the i-th of them (1≤ i≤ m) contains the maximum number of people that can go on the trip on the evening of the day i.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "4 4 2\n",
      "2 3\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "5 8 2\n",
      "2 1\n",
      "4 2\n",
      "5 4\n",
      "5 2\n",
      "4 3\n",
      "5 1\n",
      "4 1\n",
      "3 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "5\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "5 7 2\n",
      "1 5\n",
      "3 2\n",
      "2 5\n",
      "3 4\n",
      "1 2\n",
      "5 3\n",
      "1 3\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "4\n",
      "4\n",
      "\n",
      "Note\n",
      "\n",
      "In the first example, \n",
      "\n",
      "  * 1,2,3 can go on day 3 and 4. \n",
      "\n",
      "\n",
      "\n",
      "In the second example, \n",
      "\n",
      "  * 2,4,5 can go on day 4 and 5. \n",
      "  * 1,2,4,5 can go on day 6 and 7. \n",
      "  * 1,2,3,4,5 can go on day 8. \n",
      "\n",
      "\n",
      "\n",
      "In the third example, \n",
      "\n",
      "  * 1,2,5 can go on day 5. \n",
      "  * 1,2,3,5 can go on day 6 and 7. \n"
     ]
    }
   ],
   "source": [
    "old_descriptions = {}\n",
    "new_descriptions = {}\n",
    "\n",
    "# for each entry in python3_train, map name to either \"description\" or d[\"name\"] for old and new\n",
    "\n",
    "for entry in python3_train:\n",
    "    name = entry[\"name\"]\n",
    "    if name in d:\n",
    "        old_descriptions[name] = entry[\"description\"]\n",
    "        new_descriptions[name] = d[name]\n",
    "    else:\n",
    "        print(f\"Name {name} not found in descriptions dictionary.\")\n",
    "        \n",
    "# Check if the number of entries in old_descriptions and new_descriptions are the same\n",
    "print(f\"Number of entries in old_descriptions: {len(old_descriptions)}\")\n",
    "print(f\"Number of entries in new_descriptions: {len(new_descriptions)}\")\n",
    "print(\"-----------------------------\")\n",
    "print(new_descriptions['1037_E. Trips'])\n",
    "print(\"-----------------------------\")\n",
    "print(old_descriptions['1037_E. Trips'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2989d8",
   "metadata": {},
   "source": [
    "## Get Embeddings for descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa2428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked(iterable, chunk_size):\n",
    "    for i in range(0, len(iterable), chunk_size):\n",
    "        yield iterable[i:i + chunk_size]\n",
    "        \n",
    "        \n",
    "def get_embeddings_for_programs_batch(old_programs, new_programs):\n",
    "    openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=os.environ['OPENAI_API_KEY'],\n",
    "        model_name=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    \n",
    "    names = []\n",
    "    texts = []\n",
    "    tags = []\n",
    "\n",
    "    for name, old_text in old_programs.items():\n",
    "        names.append(name)\n",
    "        texts.append(old_text)\n",
    "        tags.append('old')\n",
    "\n",
    "    for name, new_text in new_programs.items():\n",
    "        names.append(name)\n",
    "        texts.append(new_text)\n",
    "        tags.append('new')\n",
    "\n",
    "    # clean text\n",
    "    texts = [str(t).replace(\"\\n\", \" \").strip() for t in texts if t is not None and str(t).strip() != \"\"]\n",
    "    \n",
    "    if not texts:\n",
    "        raise ValueError(\"No valid texts to embed.\")\n",
    "\n",
    "    print(f\"Total texts to embed: {len(texts)}\")\n",
    "\n",
    "    all_embeddings = []\n",
    "    batch_size = 512  # safe\n",
    "\n",
    "    for batch in chunked(texts, batch_size):\n",
    "        batch_embeddings = openai_ef(batch)\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "    assert len(all_embeddings) == len(names), \"Mismatch between embeddings and names!\"\n",
    "\n",
    "\n",
    "    embeddings = {}\n",
    "\n",
    "    for name, tag, embedding in zip(names, tags, all_embeddings):\n",
    "        if name not in embeddings:\n",
    "            embeddings[name] = {}\n",
    "\n",
    "        embeddings[name][tag] = embedding\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embeddings = get_embeddings_for_programs_batch(old_descriptions, new_descriptions)\n",
    "print(f\"Number of entries in embeddings: {len(embeddings)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fdae1e",
   "metadata": {},
   "source": [
    "## Cluster Data Based On Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(embeddings.keys())\n",
    "\n",
    "def cluster_embeddings(X, method=\"kmeans\", num_clusters=100, distance_threshold=None):\n",
    "    if method == \"kmeans\":\n",
    "        clustering = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "        labels = clustering.fit_predict(X)\n",
    "    elif method == \"minibatchkmeans\":\n",
    "        clustering = MiniBatchKMeans(n_clusters=num_clusters, random_state=0)\n",
    "        labels = clustering.fit_predict(X)\n",
    "    elif method == \"agglomerative\":\n",
    "        if distance_threshold is not None:\n",
    "            clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=distance_threshold, linkage='average')\n",
    "        else:\n",
    "            clustering = AgglomerativeClustering(n_clusters=num_clusters)\n",
    "        labels = clustering.fit_predict(X)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown clustering method: {method}\")\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca53186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get clusters for embeddings (old and new separately)\n",
    "def get_clusters(embeddings, method=\"kmeans\", num_clusters=100, distance_threshold=None):\n",
    "    old_embeddings = []\n",
    "    new_embeddings = []\n",
    "    for name, embedding in embeddings.items():\n",
    "        old_embeddings.append(embedding[\"old\"])\n",
    "        new_embeddings.append(embedding[\"new\"])\n",
    "\n",
    "    old_embeddings = np.array(old_embeddings)\n",
    "    new_embeddings = np.array(new_embeddings)\n",
    "\n",
    "    old_labels = cluster_embeddings(old_embeddings, method, num_clusters, distance_threshold)\n",
    "    new_labels = cluster_embeddings(new_embeddings, method, num_clusters, distance_threshold)\n",
    "\n",
    "    return old_labels, new_labels\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe106421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters in old embeddings (agglomerative): 120\n",
      "number of clusters in new embeddings (agglomerative): 120\n",
      "Number of elements in each cluster (old):\n",
      "Cluster 5: 32 elements\n",
      "Cluster 88: 36 elements\n",
      "Cluster 77: 31 elements\n",
      "Cluster 15: 59 elements\n",
      "Cluster 64: 17 elements\n",
      "Cluster 23: 45 elements\n",
      "Cluster 24: 71 elements\n",
      "Cluster 13: 97 elements\n",
      "Cluster 2: 44 elements\n",
      "Cluster 26: 45 elements\n",
      "Cluster 30: 93 elements\n",
      "Cluster 35: 32 elements\n",
      "Cluster 40: 60 elements\n",
      "Cluster 9: 83 elements\n",
      "Cluster 7: 23 elements\n",
      "Cluster 59: 42 elements\n",
      "Cluster 17: 52 elements\n",
      "Cluster 34: 50 elements\n",
      "Cluster 62: 49 elements\n",
      "Cluster 8: 32 elements\n",
      "Cluster 69: 65 elements\n",
      "Cluster 21: 99 elements\n",
      "Cluster 31: 55 elements\n",
      "Cluster 68: 37 elements\n",
      "Cluster 99: 42 elements\n",
      "Cluster 116: 30 elements\n",
      "Cluster 49: 44 elements\n",
      "Cluster 78: 16 elements\n",
      "Cluster 48: 93 elements\n",
      "Cluster 66: 35 elements\n",
      "Cluster 92: 52 elements\n",
      "Cluster 27: 44 elements\n",
      "Cluster 56: 42 elements\n",
      "Cluster 44: 40 elements\n",
      "Cluster 32: 56 elements\n",
      "Cluster 1: 67 elements\n",
      "Cluster 57: 26 elements\n",
      "Cluster 25: 54 elements\n",
      "Cluster 29: 84 elements\n",
      "Cluster 81: 33 elements\n",
      "Cluster 33: 55 elements\n",
      "Cluster 60: 22 elements\n",
      "Cluster 96: 13 elements\n",
      "Cluster 12: 42 elements\n",
      "Cluster 80: 45 elements\n",
      "Cluster 37: 80 elements\n",
      "Cluster 97: 17 elements\n",
      "Cluster 119: 30 elements\n",
      "Cluster 72: 39 elements\n",
      "Cluster 93: 15 elements\n",
      "Cluster 113: 18 elements\n",
      "Cluster 58: 61 elements\n",
      "Cluster 11: 38 elements\n",
      "Cluster 3: 93 elements\n",
      "Cluster 65: 44 elements\n",
      "Cluster 70: 23 elements\n",
      "Cluster 54: 47 elements\n",
      "Cluster 90: 40 elements\n",
      "Cluster 52: 24 elements\n",
      "Cluster 91: 16 elements\n",
      "Cluster 79: 28 elements\n",
      "Cluster 22: 69 elements\n",
      "Cluster 16: 77 elements\n",
      "Cluster 89: 26 elements\n",
      "Cluster 20: 46 elements\n",
      "Cluster 14: 56 elements\n",
      "Cluster 39: 38 elements\n",
      "Cluster 4: 32 elements\n",
      "Cluster 75: 78 elements\n",
      "Cluster 86: 24 elements\n",
      "Cluster 63: 36 elements\n",
      "Cluster 36: 20 elements\n",
      "Cluster 38: 33 elements\n",
      "Cluster 87: 12 elements\n",
      "Cluster 83: 32 elements\n",
      "Cluster 71: 33 elements\n",
      "Cluster 43: 19 elements\n",
      "Cluster 61: 24 elements\n",
      "Cluster 106: 13 elements\n",
      "Cluster 82: 57 elements\n",
      "Cluster 42: 24 elements\n",
      "Cluster 41: 51 elements\n",
      "Cluster 19: 53 elements\n",
      "Cluster 110: 38 elements\n",
      "Cluster 100: 11 elements\n",
      "Cluster 28: 50 elements\n",
      "Cluster 0: 68 elements\n",
      "Cluster 94: 24 elements\n",
      "Cluster 117: 25 elements\n",
      "Cluster 107: 14 elements\n",
      "Cluster 118: 10 elements\n",
      "Cluster 112: 20 elements\n",
      "Cluster 103: 21 elements\n",
      "Cluster 53: 32 elements\n",
      "Cluster 101: 50 elements\n",
      "Cluster 105: 28 elements\n",
      "Cluster 85: 39 elements\n",
      "Cluster 55: 24 elements\n",
      "Cluster 67: 33 elements\n",
      "Cluster 47: 18 elements\n",
      "Cluster 10: 53 elements\n",
      "Cluster 50: 29 elements\n",
      "Cluster 18: 40 elements\n",
      "Cluster 73: 29 elements\n",
      "Cluster 111: 13 elements\n",
      "Cluster 6: 40 elements\n",
      "Cluster 108: 18 elements\n",
      "Cluster 84: 26 elements\n",
      "Cluster 51: 10 elements\n",
      "Cluster 102: 12 elements\n",
      "Cluster 114: 20 elements\n",
      "Cluster 109: 25 elements\n",
      "Cluster 74: 16 elements\n",
      "Cluster 98: 9 elements\n",
      "Cluster 95: 18 elements\n",
      "Cluster 76: 19 elements\n",
      "Cluster 45: 19 elements\n",
      "Cluster 46: 20 elements\n",
      "Cluster 115: 11 elements\n",
      "Cluster 104: 14 elements\n",
      "Number of elements in each cluster (new):\n",
      "Cluster 92: 12 elements\n",
      "Cluster 91: 55 elements\n",
      "Cluster 31: 47 elements\n",
      "Cluster 23: 54 elements\n",
      "Cluster 8: 29 elements\n",
      "Cluster 76: 52 elements\n",
      "Cluster 38: 66 elements\n",
      "Cluster 102: 42 elements\n",
      "Cluster 36: 37 elements\n",
      "Cluster 58: 31 elements\n",
      "Cluster 54: 36 elements\n",
      "Cluster 52: 29 elements\n",
      "Cluster 45: 40 elements\n",
      "Cluster 13: 65 elements\n",
      "Cluster 87: 40 elements\n",
      "Cluster 89: 12 elements\n",
      "Cluster 67: 60 elements\n",
      "Cluster 44: 42 elements\n",
      "Cluster 116: 16 elements\n",
      "Cluster 11: 35 elements\n",
      "Cluster 100: 25 elements\n",
      "Cluster 78: 35 elements\n",
      "Cluster 32: 54 elements\n",
      "Cluster 60: 38 elements\n",
      "Cluster 21: 53 elements\n",
      "Cluster 15: 84 elements\n",
      "Cluster 27: 69 elements\n",
      "Cluster 118: 43 elements\n",
      "Cluster 41: 45 elements\n",
      "Cluster 10: 73 elements\n",
      "Cluster 93: 30 elements\n",
      "Cluster 34: 36 elements\n",
      "Cluster 0: 60 elements\n",
      "Cluster 16: 63 elements\n",
      "Cluster 74: 45 elements\n",
      "Cluster 64: 66 elements\n",
      "Cluster 107: 35 elements\n",
      "Cluster 66: 26 elements\n",
      "Cluster 85: 42 elements\n",
      "Cluster 53: 28 elements\n",
      "Cluster 48: 34 elements\n",
      "Cluster 73: 20 elements\n",
      "Cluster 18: 59 elements\n",
      "Cluster 2: 56 elements\n",
      "Cluster 99: 21 elements\n",
      "Cluster 104: 34 elements\n",
      "Cluster 63: 25 elements\n",
      "Cluster 43: 49 elements\n",
      "Cluster 25: 49 elements\n",
      "Cluster 6: 47 elements\n",
      "Cluster 5: 56 elements\n",
      "Cluster 1: 41 elements\n",
      "Cluster 24: 65 elements\n",
      "Cluster 26: 58 elements\n",
      "Cluster 114: 33 elements\n",
      "Cluster 3: 82 elements\n",
      "Cluster 33: 43 elements\n",
      "Cluster 12: 57 elements\n",
      "Cluster 46: 37 elements\n",
      "Cluster 77: 41 elements\n",
      "Cluster 9: 27 elements\n",
      "Cluster 39: 42 elements\n",
      "Cluster 71: 43 elements\n",
      "Cluster 88: 45 elements\n",
      "Cluster 105: 12 elements\n",
      "Cluster 30: 40 elements\n",
      "Cluster 109: 68 elements\n",
      "Cluster 20: 33 elements\n",
      "Cluster 98: 13 elements\n",
      "Cluster 19: 50 elements\n",
      "Cluster 95: 42 elements\n",
      "Cluster 14: 57 elements\n",
      "Cluster 117: 34 elements\n",
      "Cluster 50: 55 elements\n",
      "Cluster 29: 41 elements\n",
      "Cluster 72: 57 elements\n",
      "Cluster 86: 26 elements\n",
      "Cluster 40: 41 elements\n",
      "Cluster 61: 35 elements\n",
      "Cluster 69: 38 elements\n",
      "Cluster 49: 47 elements\n",
      "Cluster 79: 38 elements\n",
      "Cluster 56: 36 elements\n",
      "Cluster 17: 20 elements\n",
      "Cluster 97: 33 elements\n",
      "Cluster 119: 11 elements\n",
      "Cluster 108: 17 elements\n",
      "Cluster 22: 45 elements\n",
      "Cluster 4: 39 elements\n",
      "Cluster 94: 28 elements\n",
      "Cluster 83: 16 elements\n",
      "Cluster 101: 33 elements\n",
      "Cluster 80: 24 elements\n",
      "Cluster 112: 11 elements\n",
      "Cluster 115: 45 elements\n",
      "Cluster 90: 34 elements\n",
      "Cluster 82: 39 elements\n",
      "Cluster 68: 31 elements\n",
      "Cluster 70: 28 elements\n",
      "Cluster 42: 30 elements\n",
      "Cluster 84: 26 elements\n",
      "Cluster 59: 30 elements\n",
      "Cluster 28: 37 elements\n",
      "Cluster 103: 10 elements\n",
      "Cluster 37: 30 elements\n",
      "Cluster 55: 26 elements\n",
      "Cluster 51: 33 elements\n",
      "Cluster 75: 50 elements\n",
      "Cluster 35: 32 elements\n",
      "Cluster 110: 25 elements\n",
      "Cluster 111: 27 elements\n",
      "Cluster 47: 23 elements\n",
      "Cluster 57: 32 elements\n",
      "Cluster 81: 20 elements\n",
      "Cluster 62: 26 elements\n",
      "Cluster 106: 13 elements\n",
      "Cluster 7: 51 elements\n",
      "Cluster 96: 11 elements\n",
      "Cluster 113: 15 elements\n",
      "Cluster 65: 18 elements\n",
      "Average number of elements in old clusters (agglomerative): 38.3\n",
      "Average number of elements in new clusters (agglomerative): 38.3\n"
     ]
    }
   ],
   "source": [
    "clusters_old_aglomerative, clusters_new_agglomerative = get_clusters(embeddings, method=\"agglomerative\", num_clusters=120)\n",
    "# clusters_old_aglomerative, clusters_new_agglomerative = get_clusters(embeddings, method=\"agglomerative\", distance_threshold=0.64)\n",
    "print(f\"Number of clusters in old embeddings (agglomerative): {len(set(clusters_old_aglomerative))}\")\n",
    "print(\"number of clusters in new embeddings (agglomerative):\", len(set(clusters_new_agglomerative)))\n",
    "# print avg number of elements in clusters, average\n",
    "print(\"Number of elements in each cluster (old):\")\n",
    "old_counter_agglomerative = Counter(clusters_old_aglomerative)\n",
    "for cluster, count in old_counter_agglomerative.items():\n",
    "    print(f\"Cluster {cluster}: {count} elements\")\n",
    "print(\"Number of elements in each cluster (new):\")\n",
    "new_counter_agglomerative = Counter(clusters_new_agglomerative)\n",
    "for cluster, count in new_counter_agglomerative.items():\n",
    "    print(f\"Cluster {cluster}: {count} elements\")\n",
    "# print number of elements in clusters, average\n",
    "old_avg_agglomerative = sum(old_counter_agglomerative.values()) / len(old_counter_agglomerative)\n",
    "new_avg_agglomerative = sum(new_counter_agglomerative.values()) / len(new_counter_agglomerative)\n",
    "print(f\"Average number of elements in old clusters (agglomerative): {old_avg_agglomerative}\")\n",
    "print(f\"Average number of elements in new clusters (agglomerative): {new_avg_agglomerative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c732910d",
   "metadata": {},
   "source": [
    "## Pick Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1edaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading name descriptions...\n",
      "Loading Code Contests dataset...\n",
      "Dataset loaded.\n",
      "\n",
      "Running problem selection using clustered seeds and dual clustered neighbors...\n",
      "Selecting anchors from NEW clusters, finding neighbors in respective clusters...\n",
      "Embedding shapes: New (4596, 1536), Old (4596, 1536)\n",
      "Eligible NEW clusters (size >= 30): [34, 60, 49, 19, 48, 1, 46, 32, 5, 13, 98, 71, 11, 44, 16, 70, 33, 39, 25, 58, 14, 12, 38, 63, 10, 84, 83, 15, 28, 2, 35, 65, 51, 21, 8, 47, 57, 89, 73, 29, 76, 18, 37, 50, 31, 24, 56, 0, 23, 40, 3, 30, 61, 59, 4, 72, 97, 74, 20, 99, 66, 96, 26, 67, 94, 68, 9, 22, 7, 6, 52, 79, 41, 53, 36, 69, 87]\n",
      "Selected NEW cluster IDs to sample anchors from: [57, 18, 8, 1, 26, 3]\n",
      "\n",
      "Processing NEW Cluster ID: 57\n",
      "  Anchor: '75_D. Big Maximum Sum' (Index: 1315, Belongs to OLD Cluster: 43)\n",
      "  Finding top 30 similar using NEW embeddings within NEW Cluster 57...\n",
      "  Finding top 30 similar using OLD embeddings within OLD Cluster 43...\n",
      "\n",
      "Processing NEW Cluster ID: 18\n",
      "  Anchor: '1038_E. Maximum Matching' (Index: 4494, Belongs to OLD Cluster: 37)\n",
      "  Finding top 30 similar using NEW embeddings within NEW Cluster 18...\n",
      "  Finding top 30 similar using OLD embeddings within OLD Cluster 37...\n",
      "\n",
      "Processing NEW Cluster ID: 8\n",
      "  Anchor: '1512_A. Spy Detected!' (Index: 1953, Belongs to OLD Cluster: 8)\n",
      "  Finding top 30 similar using NEW embeddings within NEW Cluster 8...\n",
      "  Finding top 30 similar using OLD embeddings within OLD Cluster 8...\n",
      "\n",
      "Processing NEW Cluster ID: 1\n",
      "  Anchor: '1234_C. Pipes' (Index: 1113, Belongs to OLD Cluster: 33)\n",
      "  Finding top 30 similar using NEW embeddings within NEW Cluster 1...\n",
      "  Finding top 30 similar using OLD embeddings within OLD Cluster 33...\n",
      "\n",
      "Processing NEW Cluster ID: 26\n",
      "  Anchor: '390_B. Inna, Dima and Song' (Index: 2568, Belongs to OLD Cluster: 17)\n",
      "  Finding top 30 similar using NEW embeddings within NEW Cluster 26...\n",
      "  Finding top 30 similar using OLD embeddings within OLD Cluster 17...\n",
      "\n",
      "Processing NEW Cluster ID: 3\n",
      "  Anchor: '289_E. Polo the Penguin and XOR operation' (Index: 2274, Belongs to OLD Cluster: 44)\n",
      "  Finding top 30 similar using NEW embeddings within NEW Cluster 3...\n",
      "  Finding top 30 similar using OLD embeddings within OLD Cluster 44...\n",
      "\n",
      "Saving selected problem data...\n",
      "Saving data based on anchors from NEW clusters: [1, 3, 8, 18, 26, 57]\n",
      "--------------------\n",
      "Processing anchor from NEW Cluster 1 (saving as index 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully saved 30 problems to 0.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully saved 30 problems to 0.jsonl\n",
      "--------------------\n",
      "Processing anchor from NEW Cluster 3 (saving as index 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully saved 30 problems to 1.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully saved 30 problems to 1.jsonl\n",
      "--------------------\n",
      "Processing anchor from NEW Cluster 8 (saving as index 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully saved 30 problems to 2.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully saved 30 problems to 2.jsonl\n",
      "--------------------\n",
      "Processing anchor from NEW Cluster 18 (saving as index 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully saved 30 problems to 3.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully saved 30 problems to 3.jsonl\n",
      "--------------------\n",
      "Processing anchor from NEW Cluster 26 (saving as index 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully saved 30 problems to 4.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully saved 30 problems to 4.jsonl\n",
      "--------------------\n",
      "Processing anchor from NEW Cluster 57 (saving as index 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully saved 30 problems to 5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully saved 30 problems to 5.jsonl\n",
      "\n",
      "Script finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine_similarity\n",
    "\n",
    "def cosine_similarity(vec1, vec_array):\n",
    "    vec1_reshaped = vec1.reshape(1, -1)\n",
    "    sim = sklearn_cosine_similarity(vec1_reshaped, vec_array)\n",
    "    return sim[0]\n",
    "\n",
    "def pick_similar_problems_clustered_dual_assignments(\n",
    "    embeddings,\n",
    "    clusters_new, # Assignments based on new embeddings\n",
    "    clusters_old, # Assignments based on old embeddings\n",
    "    num_clusters_to_pick=6,\n",
    "    min_cluster_size=50, \n",
    "    num_similar=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Picks anchors from eligible NEW clusters. Finds neighbors using:\n",
    "    1) NEW embeddings within the anchor's NEW cluster.\n",
    "    2) OLD embeddings within the anchor's OLD cluster.\n",
    "\n",
    "    Args:\n",
    "        embeddings (dict): {'name': {'new': vec, 'old': vec}}\n",
    "        clusters_new (list/np.array): Cluster IDs from NEW embeddings.\n",
    "        clusters_old (list/np.array): Cluster IDs from OLD embeddings.\n",
    "                                     Must align with embeddings & clusters_new.\n",
    "        num_clusters_to_pick (int): How many NEW clusters to sample anchors from.\n",
    "        min_cluster_size (int): Min size for a NEW cluster to be eligible.\n",
    "        num_similar (int): How many neighbors to find (incl. anchor).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (results_new, results_old)\n",
    "               Dicts mapping the selected NEW cluster ID to the list of problem names.\n",
    "               results_new[new_cid] = neighbors found via new emb/new cluster.\n",
    "               results_old[new_cid] = neighbors found via old emb/anchor's old cluster.\n",
    "               Returns (None, None) on critical error.\n",
    "    \"\"\"\n",
    "    print(\"Selecting anchors from NEW clusters, finding neighbors in respective clusters...\")\n",
    "\n",
    "    all_names = list(embeddings.keys())\n",
    "    num_total_problems = len(all_names)\n",
    "\n",
    "    # --- Input Validation ---\n",
    "    if num_total_problems == 0:\n",
    "        print(\"Error: Embeddings dictionary is empty.\")\n",
    "        return None, None\n",
    "    if len(clusters_new) != num_total_problems:\n",
    "        print(f\"Error: Length mismatch between embeddings ({num_total_problems}) and clusters_new ({len(clusters_new)}).\")\n",
    "        return None, None\n",
    "    if len(clusters_old) != num_total_problems:\n",
    "        print(f\"Error: Length mismatch between embeddings ({num_total_problems}) and clusters_old ({len(clusters_old)}).\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        all_embeddings_new = np.array([embeddings[name][\"new\"] for name in all_names])\n",
    "        all_embeddings_old = np.array([embeddings[name][\"old\"] for name in all_names])\n",
    "        print(f\"Embedding shapes: New {all_embeddings_new.shape}, Old {all_embeddings_old.shape}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Missing embedding type ({e}).\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating embedding arrays: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    clusters_new_np = np.array(clusters_new)\n",
    "    clusters_old_np = np.array(clusters_old)\n",
    "\n",
    "\n",
    "    # --- Step 1: Identify eligible NEW clusters ---\n",
    "    cluster_counter_new = Counter(clusters_new_np)\n",
    "    eligible_new_clusters = [cid for cid, count in cluster_counter_new.items() if count >= min_cluster_size]\n",
    "\n",
    "    print(f\"Eligible NEW clusters (size >= {min_cluster_size}): {eligible_new_clusters}\")\n",
    "\n",
    "    if not eligible_new_clusters:\n",
    "         print(f\"Error: No NEW clusters found with size >= {min_cluster_size}.\")\n",
    "         return {}, {} # Return empty dicts\n",
    "    if len(eligible_new_clusters) < num_clusters_to_pick:\n",
    "        print(f\"Warning: Only {len(eligible_new_clusters)} eligible NEW clusters found. Selecting all.\")\n",
    "        num_clusters_to_pick = len(eligible_new_clusters)\n",
    "\n",
    "    selected_new_cluster_ids = random.sample(eligible_new_clusters, num_clusters_to_pick)\n",
    "    print(f\"Selected NEW cluster IDs to sample anchors from: {selected_new_cluster_ids}\")\n",
    "\n",
    "    results_new = {}\n",
    "    results_old = {}\n",
    "    processed_new_clusters = [] \n",
    "\n",
    "    for new_cluster_id in selected_new_cluster_ids:\n",
    "        print(f\"\\nProcessing NEW Cluster ID: {new_cluster_id}\")\n",
    "\n",
    "        # Find indices belonging to this selected NEW cluster\n",
    "        indices_in_new_cluster = np.where(clusters_new_np == new_cluster_id)[0]\n",
    "\n",
    "        if len(indices_in_new_cluster) < 1: # Should not happen if eligibility check worked\n",
    "            print(f\"Warning: No problems found for eligible NEW cluster {new_cluster_id}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Choose anchor from within this NEW cluster\n",
    "        anchor_idx = random.choice(indices_in_new_cluster)\n",
    "        anchor_name = all_names[anchor_idx]\n",
    "        anchor_old_cluster_id = clusters_old_np[anchor_idx] # Find anchor's OLD cluster ID\n",
    "        print(f\"  Anchor: '{anchor_name}' (Index: {anchor_idx}, Belongs to OLD Cluster: {anchor_old_cluster_id})\")\n",
    "\n",
    "        # Get anchor embeddings\n",
    "        anchor_embedding_new = all_embeddings_new[anchor_idx]\n",
    "        anchor_embedding_old = all_embeddings_old[anchor_idx]\n",
    "\n",
    "        # --- 1. Find neighbors using NEW embeddings in NEW cluster ---\n",
    "        try:\n",
    "            print(f\"  Finding top {num_similar} similar using NEW embeddings within NEW Cluster {new_cluster_id}...\")\n",
    "            # Get embeddings ONLY for problems in the anchor's NEW cluster\n",
    "            embeddings_in_new_cluster = all_embeddings_new[indices_in_new_cluster]\n",
    "\n",
    "            if len(indices_in_new_cluster) < num_similar:\n",
    "                print(f\"    Warning: NEW Cluster {new_cluster_id} has {len(indices_in_new_cluster)} members, less than {num_similar}. Returning all members.\")\n",
    "                num_similar_new = len(indices_in_new_cluster)\n",
    "            else:\n",
    "                num_similar_new = num_similar\n",
    "\n",
    "            similarities_new = cosine_similarity(anchor_embedding_new, embeddings_in_new_cluster)\n",
    "            # Indices relative to embeddings_in_new_cluster / indices_in_new_cluster\n",
    "            sorted_relative_indices_new = np.argsort(-similarities_new)\n",
    "            # Map back to original indices\n",
    "            top_original_indices_new = indices_in_new_cluster[sorted_relative_indices_new[:num_similar_new]]\n",
    "            selected_names_new = [all_names[i] for i in top_original_indices_new]\n",
    "            results_new[new_cluster_id] = selected_names_new\n",
    "            # print(f\"    Selected (New): {selected_names_new[:5]}...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Error during NEW similarity search for anchor {anchor_name}: {e}\")\n",
    "            results_new[new_cluster_id] = [] # Indicate failure for this part\n",
    "\n",
    "\n",
    "        # --- 2. Find neighbors using OLD embeddings in anchor's OLD cluster ---\n",
    "        try:\n",
    "            print(f\"  Finding top {num_similar} similar using OLD embeddings within OLD Cluster {anchor_old_cluster_id}...\")\n",
    "            # Find indices belonging to the anchor's OLD cluster\n",
    "            indices_in_old_cluster = np.where(clusters_old_np == anchor_old_cluster_id)[0]\n",
    "\n",
    "            if len(indices_in_old_cluster) == 0:\n",
    "                 print(f\"    Warning: Anchor '{anchor_name}'s OLD cluster {anchor_old_cluster_id} seems empty (error?). Skipping old neighbor search.\")\n",
    "                 results_old[new_cluster_id] = []\n",
    "                 continue # Skip to next anchor\n",
    "\n",
    "            # Get embeddings ONLY for problems in the anchor's OLD cluster\n",
    "            embeddings_in_old_cluster = all_embeddings_old[indices_in_old_cluster]\n",
    "\n",
    "            if len(indices_in_old_cluster) < num_similar:\n",
    "                print(f\"    Warning: Anchor's OLD Cluster {anchor_old_cluster_id} has {len(indices_in_old_cluster)} members, less than {num_similar}. Returning all members.\")\n",
    "                num_similar_old = len(indices_in_old_cluster)\n",
    "            else:\n",
    "                num_similar_old = num_similar\n",
    "\n",
    "            similarities_old = cosine_similarity(anchor_embedding_old, embeddings_in_old_cluster)\n",
    "            # Indices relative to embeddings_in_old_cluster / indices_in_old_cluster\n",
    "            sorted_relative_indices_old = np.argsort(-similarities_old)\n",
    "            # Map back to original indices\n",
    "            top_original_indices_old = indices_in_old_cluster[sorted_relative_indices_old[:num_similar_old]]\n",
    "            selected_names_old = [all_names[i] for i in top_original_indices_old]\n",
    "            results_old[new_cluster_id] = selected_names_old # Store using NEW cluster ID as key\n",
    "            # print(f\"    Selected (Old): {selected_names_old[:5]}...\")\n",
    "\n",
    "            # Only mark as fully processed if both parts succeeded (or handled gracefully)\n",
    "            processed_new_clusters.append(new_cluster_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Error during OLD similarity search for anchor {anchor_name}: {e}\")\n",
    "            results_old[new_cluster_id] = [] # Indicate failure for this part\n",
    "\n",
    "    # Final filtering to ensure consistency if needed (optional, depends on error handling)\n",
    "    final_results_new = {cid: results_new.get(cid, []) for cid in processed_new_clusters}\n",
    "    final_results_old = {cid: results_old.get(cid, []) for cid in processed_new_clusters}\n",
    "\n",
    "\n",
    "    return final_results_new, final_results_old\n",
    "\n",
    "\n",
    "print(\"Loading name descriptions...\")\n",
    "name2desc = {}\n",
    "desc_file_path = \"LLM_descriptions.jsonl\"\n",
    "try:\n",
    "    with open(desc_file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            x = json.loads(line)\n",
    "            name2desc[x[\"name\"]] = x.get(\"short_description\", x.get(\"description\", \"\"))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Description file not found at {desc_file_path}. Short descriptions will be empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading description file {desc_file_path}: {e}\")\n",
    "\n",
    "\n",
    "print(\"Loading Code Contests dataset...\")\n",
    "try:\n",
    "    dataset = datasets.load_dataset(\"deepmind/code_contests\", split='train')\n",
    "    print(\"Dataset loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset 'deepmind/code_contests': {e}\")\n",
    "    print(\"Cannot proceed without the dataset.\")\n",
    "    dataset = None\n",
    "\n",
    "def save_selected_problem_data(problem_names: list[str], dataset, name2desc_map: dict, output_file: Path):\n",
    "    \"\"\" Saves full data for selected problems to a JSONL file. (Definition from previous answer) \"\"\"\n",
    "    selected_set = set(problem_names)\n",
    "    examples_to_save = []\n",
    "    found_count = 0\n",
    "\n",
    "    if not dataset:\n",
    "        print(f\"Error: Dataset not loaded. Cannot save data to {output_file}\")\n",
    "        return\n",
    "    \n",
    "    for sample in tqdm(dataset, desc=f\"Saving {output_file.name}\", leave=False, ncols=100):\n",
    "        sample_name = sample.get(\"name\")\n",
    "        if not sample_name or sample_name not in selected_set:\n",
    "            continue\n",
    "\n",
    "        found_count += 1\n",
    "        solutions = sample.get(\"solutions\", {}).get(\"solution\", [])\n",
    "        languages = sample.get(\"solutions\", {}).get(\"language\", [])\n",
    "        python_solution = None\n",
    "        for i, lang in enumerate(languages):\n",
    "            if lang == 3:\n",
    "                if i < len(solutions):\n",
    "                     python_solution = solutions[i]\n",
    "                     break\n",
    "\n",
    "        entry = {\n",
    "            \"name\": sample_name,\n",
    "            \"description\": sample.get(\"description\", \"\"),\n",
    "            \"solution\": python_solution if python_solution is not None else \"PYTHON_SOLUTION_NOT_FOUND\",\n",
    "            \"difficulty\": sample.get(\"difficulty\", -1),\n",
    "            \"public_tests\": sample.get(\"public_tests\", {}),\n",
    "            \"private_tests\": sample.get(\"private_tests\", {}),\n",
    "            \"generated_tests\": sample.get(\"generated_tests\", {}),\n",
    "            \"short_description\": name2desc_map.get(sample_name, \"\"),\n",
    "        }\n",
    "        examples_to_save.append(entry)\n",
    "\n",
    "        if found_count == len(selected_set):\n",
    "             break\n",
    "\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "            for sample in examples_to_save:\n",
    "                json.dump(sample, f)\n",
    "                f.write(\"\\n\")\n",
    "        print(f\"  Successfully saved {len(examples_to_save)} problems to {output_file.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error writing to {output_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cde8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base_dir = Path(\"./clustered_removed_empty_labels\") # Change this to your desired output directory\n",
    "\n",
    "old_output_dir = output_base_dir / \"old\"\n",
    "new_output_dir = output_base_dir / \"new\"\n",
    "\n",
    "old_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "new_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"\\nRunning problem selection using clustered seeds and dual clustered neighbors...\")\n",
    "try:\n",
    "    # Define parameters, CHANGE THIS IF YOU WANT\n",
    "    num_clusters = 10\n",
    "    min_size = 30 # Minimum size for a cluster to be eligible\n",
    "    num_sim = 30 # Number of similar problems to find per anchor\n",
    "\n",
    "    # Call the new function with BOTH cluster assignments\n",
    "    selected_new_sim, selected_old_sim = pick_similar_problems_clustered_dual_assignments(\n",
    "        embeddings,\n",
    "        clusters_new=clusters_new_agglomerative, # NEW cluster assignments\n",
    "        clusters_old=clusters_old_aglomerative, # OLD cluster assignments\n",
    "        num_clusters_to_pick=num_clusters,\n",
    "        min_cluster_size=min_size,\n",
    "        num_similar=num_sim\n",
    "    )\n",
    "\n",
    "    if selected_new_sim is not None and selected_old_sim is not None:\n",
    "        print(\"\\nSaving selected problem data...\")\n",
    "        processed_new_cluster_ids = sorted(list(selected_new_sim.keys()))\n",
    "\n",
    "        if not processed_new_cluster_ids:\n",
    "            print(\"No NEW cluster anchors were successfully processed to save.\")\n",
    "        else:\n",
    "            print(f\"Saving data based on anchors from NEW clusters: {processed_new_cluster_ids}\")\n",
    "            # We use enumerate to get 0, 1, 2... for file names\n",
    "            for i, new_cluster_id in enumerate(processed_new_cluster_ids):\n",
    "                print(\"-\" * 20)\n",
    "                print(f\"Processing anchor from NEW Cluster {new_cluster_id} (saving as index {i})\")\n",
    "\n",
    "                new_file_path = new_output_dir / f\"{i}.jsonl\"\n",
    "                old_file_path = old_output_dir / f\"{i}.jsonl\"\n",
    "\n",
    "                problem_names_new = selected_new_sim.get(new_cluster_id, []) \n",
    "                problem_names_old = selected_old_sim.get(new_cluster_id, []) \n",
    "\n",
    "                if problem_names_new:\n",
    "                    save_selected_problem_data(problem_names_new, dataset, name2desc, new_file_path)\n",
    "                else:\n",
    "                    print(f\"  No 'new' neighbors found or error for anchor from NEW cluster {new_cluster_id}. Skipping save for {new_file_path.name}\")\n",
    "\n",
    "                if problem_names_old:\n",
    "                    save_selected_problem_data(problem_names_old, dataset, name2desc, old_file_path)\n",
    "                else:\n",
    "                     print(f\"  No 'old' neighbors found or error for anchor from NEW cluster {new_cluster_id}. Skipping save for {old_file_path.name}\")\n",
    "    else:\n",
    "        print(\"Problem selection function returned an error. Cannot save results.\")\n",
    "\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError: {e}\")\n",
    "except NameError as e:\n",
    "    print(f\"NameError: Required variable likely not defined ({e})\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
